---
title: "Grammar Inference with the TTT Algorithm"
author: "Martin Ong"
date: "2024-8-2"
format: 
  html:
    toc: true
    embed-resources: true
    code-fold: false
    code-tools: true
    code-line-numbers: true
    number-sections: true
bibliography: [grammar-inference.bib]
---
# Prerequisites

```{python}
from __future__ import annotations
from typing import Iterable, Optional, Any, Callable, TypeVar, Protocol, cast
from pprint import pprint
from collections import defaultdict
import heapq
```

# Observation Pack Algorithm
Observation Pack is an algorithm for inferring regular grammars.
It is able to infer grammars in a black-box context with the help of a
teacher which answers membership and equivalence queries.

Observation Pack is an optimisation of the L\* algorithm [@angluin_learning_1987],
focusing on reducing the amount of redundant membership queries that are performed.
As in L\*, states in the hypothesis correspond to a prefix-closed set,
called the short prefixes, however Observation Pack improves on L\*
by dropping the requirement for all suffixes to be evaluated for each state
to distinguish them. <!-- TODO: REWORD -->

## Notation
somehting about aseq

## Teacher
In order to learn a regular grammar, a Teacher is used which can answer membership
and equivalence queries. An interface can be defined as follows:

```{python}
class HypothesisProtocol(Protocol):
    def evaluate(self, u: str) -> bool:
        ...


class TeacherProtocol(Protocol):
    def is_member(self, s: str) -> bool:
        ...

    def is_equivalent(self, h: HypothesisProtocol) -> tuple[bool, Optional[str]]:
        ...
```

Membership queries are straightforward to implement, as the teacher can simply
run the black-box program on the input string. However, equivalence queries are
not so simple. Angluin showed that equivalence queries can be approximated through
repeated membership queries in [@angluin_learning_1987], so that is the approach
we will use here. In particular, she showed that if $i$ previous equivalence queries
have been performed, then the number of membership queries to perform is given by
$$\lceil \frac{1}{\varepsilon}\biggl(\log \frac{1}{\delta} + (\log 2)(i+1)\biggr) \rceil,$$
where $\varepsilon, \delta$ are the accuracy and confidence parameters.

```{python}
class SimpleTeacher(Teacher):
    alphabet: str
    regex: Pattern[str]
    equivalence_query_counter: int
    epsilon: float
    delta: float

    def __init__(self, alphabet: str, pattern: str, accuracy: float = 0.1,
                confidence: float = 0.1, seed: int = 1):
        self.alphabet = alphabet
        self.regex = re.compile(pattern)
        self.equivalence_query_counter = 0
        self.accuracy = epsilon
        self.confidence = delta

    def is_member(self, u: str) -> bool:
        return self.regex.fullmatch(u) is not None

    def is_equivalent(self, hypothesis: HypothesisProtocol,
                        max_length: int = 10) -> tuple[bool, Optional[str]]:
        self.equivalence_query_counter += 1
        num_calls = math.ceil(1.0/self.epsilon * (math.log(1.0/self.delta) +
                              self.equivalence_query_counter * math.log(2)))

        for _ in range(num_calls):
            s = self.gen_random(max_length)
            if self.is_member(s) != hypothesis.evaluate(s):
                return False, s

        return True, None

    def gen_random(self, max_length) -> str:
        total_combinations = len(self.alphabet) ** max_length
        random_idx = random.randint(1, total_combinations)

        # TODO: just compare with precomputed values (no log needed)
        length = math.floor(math.log(random_idx, len(self.alphabet)))

        return "".join(random.choice(self.alphabet) for i in range(length))
```

## Discrimination Trees
A Discrimination Tree is a data structure which contains the minimal amount of
information required to discriminate between a set of states in a finite state machine.

A Discrimination Tree is a binary tree, in which each inner node has two children,
where the inner nodes correspond with a discriminator string and leaves correspond
with states in a finite state machine. The two children are denoted the 0-child
and the 1-child, and any leaf corresponding to a state $x$ in the o-subtree
($o \in \{0, 1\}$) satisfies $\lambda^{\mathcal{A}}(\lfloor x \rfloor v) = o$.

Each node in the tree stores its signature, which essentially gives its
position in the tree. This signature is defined as  <!-- TODO: finish sentence -->

Discrimination Trees are useful for grammar inference because they allow for states
to be (bucketed) and (through sifting) <!-- TODO: fix this -->

A discrimination tree can be realised as follows.

```{python}
class Node:
    block: Optional[Block]
    signature: set[tuple[str, bool]]

    def __init__(self, block: Optional[Block] = None,
                signature: set[tuple[str, bool]] = set()) -> None:
        self.incoming = incoming
        self.block = block
        self.signature = signature

    def __str__(self) -> str:
        return self.to_string()

    @property
    def depth(self) -> int:
        return len(self.signature)

class InnerNode(Node):
    children: list[Node]
    parent: Optional[InnerNode]
    discriminator: str

    def __init__(self, zero_child: Node, one_child: Node, discriminator: str,
                    parent = None, incoming = set(), block = None,
                    signature: set[tuple[str, bool]] = set()) -> None:
        self.children = [zero_child, one_child]
        self.parent = parent
        self.discriminator = discriminator
        super.__init__(incoming, block, signature)


class LeafNode(Node):
    parent: InnerNode
    state: Optional[State]

    def __init__(self, parent, state = None, incoming = set(), block = None,
                    signature: set[tuple[str, bool]] = set()) -> None:
        self.parent = parent
        self.state = state
        super.__init__(incoming, block, signature)
```

Since each inner node stores a string which discriminates between any leaves in
its 0 and 1 subtrees, a discriminator for any two leaves can be computed by
finding the lowest common ancestor of the leaves.

Moreover, for any set of states, a discriminator which essentially splits
the set in two <!-- TODO: reword --> can be computed as the lowest common
ancestor of the set of states.

```{python}
class Node(Node):
    @classmethod
    def lca(cls, nodes: list[Node]) -> Node:
        min_depth = min(map(lambda node: node.depth, nodes))
        nodes_in_layer: set[Node] = set()

        for node in nodes:
            while node.depth > min_depth:
                node = node.parent

                if node is None:
                    raise ValueError("Null parent of non-root node")

            nodes_in_layer.add(node)

        while len(nodes_in_layer) > 1:
            nodes_in_layer = {
                node.parent for node in nodes_in_layer if node.parent is not None
            }

        if len(nodes_in_layer) == 0:
            raise ValueError("LCA couldn't be computed")

        return nodes_in_layer.pop()
```

Additionally, the sifting operation can be thought of as a variant of binary insertion,
where an ordering of leaves is defined in relation to whether or not they are
discriminated by the various discriminants in the tree.

```{python}
class Node(Node):
    def sift(self, teacher: TeacherProtocol, u: str) -> Node:
        if self.is_leaf():
            return self

        assert self.discriminator is not None  # self is inner

        o = teacher.is_member(u + self.discriminator)
        child = self.children[o]
        if child is None:
            raise ValueError("Null child reached in sifting")

        return child.sift(teacher, u)
```
